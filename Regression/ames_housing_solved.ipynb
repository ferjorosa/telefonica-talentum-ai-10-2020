{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ames housing dataset\n",
    "\n",
    "The <a href=\"http://jse.amstat.org/v19n3/decock.pdf\">**Ames Housing dataset**</a> was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. It is a common <a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\">**Kaggle competition**</a> that is usually used to explain advanced regression techniques.\n",
    "\n",
    "Here's a brief version of what you'll find in the data description file (<code>ames_housing_description.txt</code>).\n",
    "\n",
    "* Feature variables:\n",
    "    * **MSSubClass** : The building class\n",
    "    * **MSZoning** : The general zoning classification\n",
    "    * **LotFrontage** : Linear feet of street connected to property\n",
    "    * **LotArea** : Lot size in square feet\n",
    "    * **Street** : Type of road access\n",
    "    * **Alley** : Type of alley access\n",
    "    * **LotShape** : General shape of property\n",
    "    * **LandContour** : Flatness of the property\n",
    "    * **Utilities** : Type of utilities available\n",
    "    * **LotConfig** : Lot configuration\n",
    "    * **LandSlope** : Slope of property\n",
    "    * **Neighborhood** : Physical locations within Ames city limits\n",
    "    * **Condition1** : Proximity to main road or railroad\n",
    "    * **Condition2** : Proximity to main road or railroad (if a second is present)\n",
    "    * **BldgType** : Type of dwelling\n",
    "    * **HouseStyle** : Style of dwelling\n",
    "    * **OverallQual** : Overall material and finish quality\n",
    "    * **OverallCond** : Overall condition rating\n",
    "    * **YearBuilt** : Original construction date\n",
    "    * **YearRemodAdd** : Remodel date\n",
    "    * **RoofStyle** : Type of roof\n",
    "    * **RoofMatl** : Roof material\n",
    "    * **Exterior1st** : Exterior covering on house\n",
    "    * **Exterior2nd** : Exterior covering on house (if more than one material)\n",
    "    * **MasVnrType** : Masonry veneer type\n",
    "    * **MasVnrArea** : Masonry veneer area in square feet\n",
    "    * **ExterQual** : Exterior material quality\n",
    "    * **ExterCond** : Present condition of the material on the exterior\n",
    "    * **Foundation** : Type of foundation\n",
    "    * **BsmtQual** : Height of the basement\n",
    "    * **BsmtCond** : General condition of the basement\n",
    "    * **BsmtExposure** : Walkout or garden level basement walls\n",
    "    * **BsmtFinType1** : Quality of basement finished area\n",
    "    * **BsmtFinSF1** : Type 1 finished square feet\n",
    "    * **BsmtFinType2** : Quality of second finished area (if present)\n",
    "    * **BsmtFinSF2** : Type 2 finished square feet\n",
    "    * **BsmtUnfSF** : Unfinished square feet of basement area\n",
    "    * **TotalBsmtSF** : Total square feet of basement area\n",
    "    * **Heating** : Type of heating\n",
    "    * **HeatingQC** : Heating quality and condition\n",
    "    * **CentralAir** : Central air conditioning\n",
    "    * **Electrical** : Electrical system\n",
    "    * **1stFlrSF** : First Floor square feet\n",
    "    * **2ndFlrSF** : Second floor square feet\n",
    "    * **LowQualFinSF** : Low quality finished square feet (all floors)\n",
    "    * **GrLivArea** : Above grade (ground) living area square feet\n",
    "    * **BsmtFullBath** : Basement full bathrooms\n",
    "    * **BsmtHalfBath** : Basement half bathrooms\n",
    "    * **FullBath** : Full bathrooms above grade\n",
    "    * **HalfBath** : Half baths above grade\n",
    "    * **Bedroom** : Number of bedrooms above basement level\n",
    "    * **Kitchen** : Number of kitchens\n",
    "    * **KitchenQual** : Kitchen quality\n",
    "    * **TotRmsAbvGrd** : Total rooms above grade (does not include bathrooms)\n",
    "    * **Functional** : Home functionality rating\n",
    "    * **Fireplaces** : Number of fireplaces\n",
    "    * **FireplaceQu** : Fireplace quality\n",
    "    * **GarageType** : Garage location\n",
    "    * **GarageYrBlt** : Year garage was built\n",
    "    * **GarageFinish** : Interior finish of the garage\n",
    "    * **GarageCars** : Size of garage in car capacity\n",
    "    * **GarageArea** : Size of garage in square feet\n",
    "    * **GarageQual** : Garage quality\n",
    "    * **GarageCond** : Garage condition\n",
    "    * **PavedDrive** : Paved driveway\n",
    "    * **WoodDeckSF** : Wood deck area in square feet\n",
    "    * **OpenPorchSF** : Open porch area in square feet\n",
    "    * **EnclosedPorch** : Enclosed porch area in square feet\n",
    "    * **3SsnPorch** : Three season porch area in square feet\n",
    "    * **ScreenPorch** : Screen porch area in square feet\n",
    "    * **PoolArea** : Pool area in square feet\n",
    "    * **PoolQC** : Pool quality\n",
    "    * **Fence** : Fence quality\n",
    "    * **MiscFeature** : Miscellaneous feature not covered in other categories\n",
    "    * **MiscVal** : Value of miscellaneous feature in \\$\n",
    "    * **MoSold** : Month Sold\n",
    "    * **YrSold** : Year Sold\n",
    "    * **SaleType** : Type of sale\n",
    "    * **SaleCondition** : Condition of sale\n",
    "\n",
    "\n",
    "* Predictive variable:\n",
    "    * **SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
    "    \n",
    "#### Missing\n",
    "There are several missing values that are represented in the CSV file as \"?\"\n",
    "    \n",
    "#### Objective\n",
    "Our objective is to produce a good estimator of the house sale price. However, this time we are facing a more complex dataset. \n",
    "\n",
    "#### Table of contents\n",
    "<ol>\n",
    "    <li><a href=\"#preprocessing\">Preprocessing</a></li>\n",
    "        1.1. <a href=\"#find_missing\">Find missing values</a><br>\n",
    "        1.2. <a href=\"#impute_missing\">Impute missing values</a><br>\n",
    "    <li><a href=\"#exploratory_data_analysis\">Exploratory data analysis</a></li>\n",
    "        2.1. <a href=\"#univariate_analysis\">Univariate analysis</a><br>\n",
    "        2.2. <a href=\"#bivariate_analysis\">Bivariate analysis</a><br>\n",
    "    <li><a href=\"#regression_analysis\">Regression analysis</a></li>\n",
    "        3.1. <a href=\"#linear_regression\">Linear regression</a><br>\n",
    "        3.2. <a href=\"#lasso_regression\">Lasso regression</a><br>\n",
    "        3.3. <a href=\"#ridge_regression\">Ridge regression</a><br>\n",
    "        3.4. <a href=\"#decision_tree\">Decision tree</a><br>\n",
    "        3.5. <a href=\"#knn\">KNN</a><br>\n",
    "    <li><a href=\"#estimator_evaluation\">Estimator evaluation</a></li>\n",
    "</ol>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"../Data/ames_housing_train.csv\")\n",
    "df_test = pd.read_csv(\"../Data/ames_housing_test.csv\")\n",
    "\n",
    "df_train = df_train.drop(columns = [\"Id\"]) # Drop the ID column (ID of each house)\n",
    "df_test = df_test.drop(columns = [\"Id\"]) # Drop the ID column (ID of each house)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#preprocessing\">1 - Preprocessing</h2>\n",
    "\n",
    "The first step in our preprocessing process is to recognize numerical variables (we cannot directly use categorical variables in regression analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in df_train.columns if df_train[col].dtype == \"object\"]\n",
    "numeric_cols = [col for col in df_train.columns if df_train[col].dtype != \"object\"] # numeric variables\n",
    "df_train_numeric = df_train[numeric_cols]\n",
    "df_test_numeric = df_test[numeric_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#find_missing\">1.1 - Find missing values</h3>\n",
    "\n",
    "Before applying our regression methods, it is necessary that we check if there are missing values because they would prevent our estimators to learn their models or at least affect their results. We have to do it for both the train and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "missing_data = df_train_numeric.isnull() # Returns a DataFrame with True/False depending on the existence of a missing value\n",
    "true_counts = [(column, np.count_nonzero(missing_data[column] == True)) for column in missing_data.columns]\n",
    "false_counts = [(column, np.count_nonzero(missing_data[column] == False)) for column in missing_data.columns]\n",
    "true_counts.sort(key=lambda x:x[1], reverse = True) # Sort from more to less\n",
    "true_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "missing_data = df_test_numeric.isnull() # Returns a DataFrame with True/False depending on the existence of a missing value\n",
    "true_counts = [(column, np.count_nonzero(missing_data[column] == True)) for column in missing_data.columns]\n",
    "false_counts = [(column, np.count_nonzero(missing_data[column] == False)) for column in missing_data.columns]\n",
    "true_counts.sort(key=lambda x:x[1], reverse = True) # Sort from more to less\n",
    "true_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#impute_missing\">1.2 - Impute missing values</h3>\n",
    "\n",
    "There are several alternatives for imputation. In this case we will use the <code>IterativeImputer</code>, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # explicitly require this experimental feature\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "\n",
    "df_train_numeric_no_missing = pd.DataFrame(data = imputer.fit_transform(df_train_numeric), columns=df_train_numeric.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that missing values have been imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df_train_numeric_no_missing.isnull() # Returns a DataFrame with True/False depending on the existence of a missing value\n",
    "true_counts = [(column, np.count_nonzero(missing_data[column] == True)) for column in missing_data.columns]\n",
    "#true_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # explicitly require this experimental feature\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "\n",
    "df_test_numeric_no_missing = pd.DataFrame(data = imputer.fit_transform(df_test_numeric), columns=df_train_numeric.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#exploratory_data_analysis\">2 - Exploratory data analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_numeric_no_missing[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#univariate_analysis\">2.1 - Univariate analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(ncols=6, nrows=6, figsize=(20, 15))\n",
    "axs = axs.flatten() # \n",
    "\n",
    "index = 0\n",
    "for k,v in df_train_numeric_no_missing.drop(columns=[\"SalePrice\"]).items():\n",
    "    sns.distplot(v, bins=20, ax=axs[index])\n",
    "    index += 1\n",
    "\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Errors occur when values are very concentrated, which doesnt allow the library to properly do KDE and thus only bar diagrams are shown. In some cases, like **YrSold**, it may not even make sens to use a KDE because its discrete and there are so few distinct values that it may be eve more interesting to consider it as a categorical variable. \n",
    "\n",
    "Following on this topic, we have to be careful which variables we consider to be numeric, because sometimes it will make our model more difficult to interpret, even if it obtains a \"better score\". For example, what is a house with 2.5 rooms? This is especially important in unsupervised tasks as clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "\n",
    "x = df_train_numeric_no_missing[\"SalePrice\"]\n",
    "\n",
    "sns.distplot(x, bins = 20, rug=True);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#bivariate_analysis\">2.2 - Bivariate analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_numeric_no_missing.corr()[\"SalePrice\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corr = df_train_numeric_no_missing.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.7, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe there is one variable that seems to linearly explain **SalePrice** quite well. That variable is **OverallQual**, which represent the overall material and finish quality of the house. Interestingly, the overall condition of the house doesnt seem to affect that much on this preliminary study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "\n",
    "sns.regplot(x=df_train_numeric_no_missing[\"OverallQual\"], y=df_train_numeric_no_missing[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other interesting variable is **GrLivArea**, which represents the above ground living area square feet. It seems that the bigger, the pricier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "\n",
    "sns.regplot(x=df_train_numeric_no_missing[\"GrLivArea\"], y=df_train_numeric_no_missing[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#regression_analysis\">3 - Regression analysis</h2>\n",
    "\n",
    "In this case we don't need to do a train-test split because it has been done for us. We will simply prepare our <code>X_train</code>, <code>Y_train</code>, <code>X_test</code> and <code>Y_test</code> dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_numeric_no_missing.drop(columns=[\"SalePrice\"])\n",
    "Y_train = df_train_numeric_no_missing[\"SalePrice\"]\n",
    "X_test = df_test_numeric_no_missing.drop(columns=[\"SalePrice\"])\n",
    "Y_test = df_test_numeric_no_missing[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#linear_regression\">3.1 - Linear regression</h3>\n",
    "\n",
    "We will use <code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">sklearn.linear_model.LinearRegression</a></code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-dimensional linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train_onedim = X_train[[\"OverallQual\"]]\n",
    "X_test_onedim = X_test[[\"OverallQual\"]]\n",
    "\n",
    "scores = cross_val_score(lin_reg, X_train_onedim, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-dimensional linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(lin_reg, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that simply knowing the overall quality of the house (in terms of construction and materials) may not be enough information to correctly predict its price. \n",
    "\n",
    "**Note:** While by doing a multi-dimensional regression we obtain better results, it would be interesting to reduce the number of variables while maintaining a similar score. We could do it with a feature selection technique. Sklearn offers the possibility with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-dimensional linear regression with manual feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_reg_fs = LinearRegression()\n",
    "selected_features = [\"OverallQual\", \"GrLivArea\", \"FullBath\", \"GarageArea\"]\n",
    "scores = cross_val_score(lin_reg_fs, X_train[selected_features], Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have selected 4 variables that seem to be highly correlated with the price of the house. We have selected only these 4 variables because while other variables are also highly correlated with the price, they are also highly correlated with these 4 variables, so they would add little extra information while increasing the dimensionality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#ridge_regression\">3.2 - Ridge regression</h3>\n",
    "\n",
    "We will use <code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">sklearn.linear_model.Ridge</a></code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg_1 = Ridge(alpha=1.0) \n",
    "\n",
    "scores = cross_val_score(ridge_reg_1, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg_05 = Ridge(alpha=0.5)\n",
    "\n",
    "scores = cross_val_score(ridge_reg_05, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there isn't much difference between $\\alpha$ = 0.5 and $\\alpha$ = 1.0. we choose the second because of it slight advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#lasso_regression\">3.3 - Lasso regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg_1 = Lasso(alpha=1.0) \n",
    "\n",
    "scores = cross_val_score(lasso_reg_1, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg_05 = Lasso(alpha=0.5) \n",
    "\n",
    "scores = cross_val_score(lasso_reg_05, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there isn't much difference between $\\alpha$ = 0.5 and $\\alpha$ = 1.0. we choose the second because of it slight advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#decision_tree\">3.4 - Decision tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "scores = cross_val_score(dt_reg, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With manual feature selection\n",
    "\n",
    "We will use the same features as in the case of linear regression: **OverallQual**, **GrLivArea**, **FullBath** and **GarageArea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_reg_fs = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "scores = cross_val_score(dt_reg_fs, X_train[selected_features], Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#knn\">3.5 - KNN</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $k = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_reg_2 = KNeighborsRegressor(n_neighbors=2)\n",
    "scores = cross_val_score(knn_reg_2, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $k = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_reg_5 = KNeighborsRegressor(n_neighbors=5)\n",
    "scores = cross_val_score(knn_reg_5, X_train, Y_train, cv=5) # By default we use the R2 score\n",
    "print(\"Cross-val score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the best setting of the KNN with $k = 5$. It will be used in the final comparison with the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#exploratory_data_analysis\">4 - Estimator evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_fitted = lin_reg.fit(X_train, Y_train)\n",
    "lin_reg_fs_fitted = lin_reg_fs.fit(X_train[selected_features], Y_train)\n",
    "ridge_reg_fitted = ridge_reg_05.fit(X_train, Y_train)\n",
    "lasso_reg_fitted = lasso_reg_05.fit(X_train, Y_train)\n",
    "dt_reg_fitted = dt_reg.fit(X_train, Y_train)\n",
    "dt_reg_fs_fitted = dt_reg_fs.fit(X_train[selected_features], Y_train)\n",
    "knn_fitted = knn_reg_5.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear regression score: \" + str(lin_reg_fitted.score(X_test, Y_test)))\n",
    "print(\"Linear regression with manual FS score: \" + str(lin_reg_fs_fitted.score(X_test[selected_features], Y_test)))\n",
    "print(\"Ridge regression score: \"+ str(ridge_reg_fitted.score(X_test, Y_test)))\n",
    "print(\"Lasso regression score: \"+ str(lasso_reg_fitted.score(X_test, Y_test)))\n",
    "print(\"Decision tree score: \" + str(dt_reg_fitted.score(X_test, Y_test)))\n",
    "print(\"Decision tree manual FS score: \" + str(dt_reg_fs_fitted.score(X_test[selected_features], Y_test)))\n",
    "print(\"KNN score: \" + str(knn_fitted.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the decision tree regressor is the best estimator. Some notes:\n",
    "* There isn't much difference between the application or not of regularization in our multi-dimensional linear regression models. \n",
    "* We observe that while the linear regression model with 4 variables shows the worst results of all.\n",
    "* The decision tree obtained almost perfect score in test, even better than the cross-validated train score. this could probaly mean that those test instances where not that different from what it had previously observed and being so prone to overfitting, it wasn't difficult for it to estimate their true values. It is interesting to note that our manual feature selection with only 4 variables obtained pretty much an identical score. \n",
    "* The KNN obtains much better results in test than in train, but it is still far away from those of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(dt_reg_fitted.feature_importances_, index=X_train.columns)\n",
    "feature_importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_manual = pd.Series(dt_reg_fs_fitted.feature_importances_, index=X_train[selected_features].columns)\n",
    "feature_importances_manual.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select new variables using this information (from test data) would be dangerous, but it gives us how important are currently each of the model variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on feature selection\n",
    "\n",
    "There are <a href=\"https://en.wikipedia.org/wiki/Feature_selection#Main_principles\">three main types of feature selection</a> based on how they combine the selection algorithm and the process of model building.\n",
    "\n",
    "* **Filter approach**. Filter type methods select variables regardless of the model. \n",
    "    * They are based only on general features like the correlation or mutual information with the variable to predict.\n",
    "    * These methods are particularly effective in computation time and robust to overfitting. \n",
    "    * These methods tend to select redundant variables when they do not consider the relationships between variables. However, more elaborate features try to minimize this problem by removing variables highly related to each other, such as the mRMR algorithm of Peng et al.(2005).\n",
    "    * Several well-known filter methods are based on <a href=\"https://en.wikipedia.org/wiki/Feature_selection#Information_Theory_Based_Feature_Selection_Mechanisms\">information theory</a>.\n",
    "    * Scikit-learn doesn't provide a specific filter method for feature selection, but we could create our own by estimating the MI, NMI or correlation between variables. \n",
    "\n",
    "<img src=\"images/filter_method.png\" width=\"500\">\n",
    "\n",
    "* **Wrapper approach**. Wrapper type methods evaluate subsets of variables with respect to the predictive score.\n",
    "    * They are usually applied in combination with cross-validation. A common idea is to start with all the feature variables and then recursively prune the subset of least relevant variables.\n",
    "    * They can be applied with any estimator as long as there is a score to optimize.\n",
    "    * They are very computationally expensive.\n",
    "    * There is a higher risk of overfitting, which can be reduced with cross-validation.\n",
    "    * Scikit-learn provides an implementation for those methods with built-in feature importances such as decision trees and SVMs. See the class <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV\"><code>sklearn.feature_selection.RFECV</code></a>.\n",
    "    \n",
    "\n",
    "<img src=\"images/wrapper_method.png\" width=\"500\">\n",
    "\n",
    "* **Embedded approach**.\n",
    "    * The search for an optimal subset of features is built into the model construction.\n",
    "    * They combine the advantages of filter and wrapper approaches, but they are specific to the estimator. The estimator performs feature selection and prediction simultaneously.\n",
    "\n",
    "<img src=\"images/embedded_method.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. <a href=\"https://scikit-learn.org/stable/index.html\">Scikit-learn documentation</a>\n",
    "2. <a href=\"https://en.wikipedia.org/wiki/Feature_selection\">Feature selection (Wikipedia)</a>\n",
    "3. <a href=\"http://ranger.uta.edu/~chqding/papers/mRMR_PAMI.pdf\">Peng H., Long F. and Ding C., \"Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy\", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 8, 2005.</a>\n",
    "4. <a href=\"https://machinelearningmastery.com/rfe-feature-selection-in-python/\">Recursive Feature Elimination (RFE) for Feature Selection in Python</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
